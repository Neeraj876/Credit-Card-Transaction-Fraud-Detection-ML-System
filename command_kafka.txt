wget https://downloads.apache.org/kafka/3.9.0/kafka_2.13-3.9.0.tgz
tar -xvzf kafka_2.13-3.9.0.tgz


-----------------------
java -version
sudo yum install -y java-17-amazon-corretto
java -version
cd kafka_2.13-3.9.0

Start Zoo-keeper:
-------------------------------
bin/zookeeper-server-start.sh config/zookeeper.properties

Open another window to start kafka
But first ssh to to your ec2 machine as done above


Start Kafka-server:
----------------------------------------
Duplicate the session & enter in a new console --
export KAFKA_HEAP_OPTS="-Xmx256M -Xms128M"
cd kafka_2.13-3.9.0
bin/kafka-server-start.sh config/server.properties

It is pointing to private server , change server.properties so that it can run in public IP 

To do this , you can follow any of the 2 approaches shared below --
Do a "sudo nano config/server.properties" - change ADVERTISED_LISTENERS to public ip of the EC2 instance


Create the topic:
-----------------------------
Duplicate the session & enter in a new console --
cd kafka_2.13-3.9.0
bin/kafka-topics.sh --create --topic demo_test --bootstrap-server 54.152.28.77:9092 --replication-factor 1 --partitions 1

Start Producer:
--------------------------
bin/kafka-console-producer.sh --topic raw_transactions --bootstrap-server 54.152.28.77:9092

Start Consumer:
-------------------------
Duplicate the session & enter in a new console --
cd kafka_2.13-3.9.0
bin/kafka-console-consumer.sh --topic valid_transactions --bootstrap-server 18.207.219.175:9092


Commands to stop zookeeper and kafka server and then res:
bin/kafka-server-stop.sh
bin/zookeeper-server-stop.sh
bin/zookeeper-server-start.sh config/zookeeper.properties 
bin/kafka-server-start.sh config/server.properties 

Command to change directory:
cd /d D:\real_time_streaming

cd D:real_time_streaming

ssh -i "kafka-project.pem" ec2-user@ec2-44-203-193-110.compute-1.amazonaws.com

44.203.193.110

Three Kafka topics:

bin/kafka-topics.sh --create --topic raw_transactions --bootstrap-server 44.203.193.110:9092 --replication-factor 1 --partitions 1

bin/kafka-topics.sh --create --topic valid_transactions --bootstrap-server 44.203.193.110:9092 --replication-factor 1 --partitions 1

bin/kafka-topics.sh --create --topic invalid_transactions --bootstrap-server 44.203.193.110:9092 --replication-factor 1 --partitions 1

How to setup confluent schema registry on AWS EC2:

Steps to Allow Port 8081 in AWS Security Groups:
1. Go to AWS Console → EC2 Dashboard
2. Select Your Instance
3. Click on the Security Group linked to your instance
4. Edit Inbound Rules → Click "Add Rule"
 - Type: Custom TCP
 - Port Range: 8081
 - Source: 0.0.0.0/0 (or your IP for security)
5. Save Changes

1. Install Confluent Schema Registry on EC2

Step 1: Install Java (if not already installed)

sudo apt update
sudo apt install openjdk-11-jdk -y

Verify Java installation:
java -version

Step 2: Download Confluent Schema Registry

wget https://packages.confluent.io/archive/7.5/confluent-community-7.5.0.tar.gz
tar -xvzf confluent-community-7.5.0.tar.gz
cd confluent-7.5.0

Step 3: Configure Schema Registry

Edit the schema-registry.properties file:

nano etc/schema-registry/schema-registry.properties

Update these properties:

listeners=http://0.0.0.0:8081
kafkastore.bootstrap.servers=<YOUR_KAFKA_BROKER>:9092
kafkastore.topic=_schemas
debug=true

Replace <YOUR_KAFKA_BROKER> with the IP address of your Kafka broker.

Save and exit (CTRL+X, then Y, then Enter).

Step 4: Start Schema Registry

bin/schema-registry-start etc/schema-registry/schema-registry.properties

If it starts successfully, your Schema Registry URL will be:
http://<EC2-PUBLIC-IP>:8081

2. Register Your Avro Schema
Now that the Schema Registry is running, you need to register your .avsc schema.

Example Avro Schema (fraud_schema.avsc)

{
  "type": "record",
  "name": "Transaction",
  "fields": [
    { "name": "cc_num", "type": "int" },
    { "name": "trans_date_trans_time", "type": "string" }, 
    { "name": "merchant", "type": "string" },
    { "name": "category", "type": "string" },
    { "name": "amt", "type": "float" }, 
    { "name": "gender", "type": "string" },
    { "name": "city_pop", "type": "int" },
    { "name": "job", "type": "string" },
    { "name": "is_fraud", "type": "int" }
  ]
}


1. Register Schema via CURL

Run the following command on your EC2 instance to register your schema:

curl -X POST -H "Content-Type: application/vnd.schemaregistry.v1+json" --data '{"schema": "{\"type\":\"record\",\"name\":\"Transaction\",\"fields\":[{\"name\":\"_id\",\"type\":\"string\"},{\"name\":\"Unnamed_0\",\"type\":\"int\"},{\"name\":\"trans_date_trans_time\",\"type\":\"string\"},{\"name\":\"cc_num\",\"type\":\"long\"},{\"name\":\"merchant\",\"type\":\"string\"},{\"name\":\"category\",\"type\":\"string\"},{\"name\":\"amt\",\"type\":\"float\"},{\"name\":\"first\",\"type\":\"string\"},{\"name\":\"last\",\"type\":\"string\"},{\"name\":\"gender\",\"type\":\"string\"},{\"name\":\"street\",\"type\":\"string\"},{\"name\":\"city\",\"type\":\"string\"},{\"name\":\"state\",\"type\":\"string\"},{\"name\":\"zip\",\"type\":\"int\"},{\"name\":\"lat\",\"type\":\"double\"},{\"name\":\"long\",\"type\":\"double\"},{\"name\":\"city_pop\",\"type\":\"int\"},{\"name\":\"job\",\"type\":\"string\"},{\"name\":\"dob\",\"type\":\"string\"},{\"name\":\"trans_num\",\"type\":\"string\"},{\"name\":\"unix_time\",\"type\":\"int\"},{\"name\":\"merch_lat\",\"type\":\"double\"},{\"name\":\"merch_long\",\"type\":\"double\"},{\"name\":\"is_fraud\",\"type\":\"int\"}]}"}' \
http://44.203.193.110:8081/subjects/raw_transactions-value/versions

This should return:

{"id":1}

2. Verify Schema Registration

Check if the schema is registered:

1. curl -X GET http://<EC2-PUBLIC-IP>:8081/subjects

This should return:

["<kaka-topicname>-value"]

Your schema name (subject) is user-value.

2. curl -X GET http://107.20.106.196:8081/subjects/<kaka-topicname>-value/versions

This should return:

[1]

# Command to run a fresher redis container and to allow external connections

docker run --name redis -p 6379:6379 -d redis --bind 0.0.0.0

# Command to test the connection 

redis-cli -h 127.0.0.1 -p 6379 ping

# Command to check fastapi is recieveing request or not from spark

curl -X POST http://127.0.0.1:8000/transaction -H "Content-Type: application/json" -d '{"transaction_id": 77000000}'

# Feast materialize command

feast materialize-incremental $(date -u +"%Y-%m-%dT%H:%M:%S")